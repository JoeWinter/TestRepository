<div xmlns="http://www.w3.org/1999/xhtml">
  <div style="text-align: left;">[[TableOfContents|Table of Contents]] | [[Minimal_Configuration|Previous]] | [[Expanding_the_Cluster|Next]] | [[Index|Index]]</div>
  <div style="color: #999999; font-family: sans-serif; font-size: 10pt; text-align: left;">
    <span>[[Deployment_Guidelines|Deployment Guidelines]]</span> : Two Node Configuration</div>
  <hr />
  <div style="color: #345A8A; font-family: &quot;Museo For Dell Regular&quot;; font-size: 13pt; font-weight: bold; margin-bottom: 12pt; margin-top: 12pt;"><span id="wwpID0E0EO0HA">Two Node Configuration</span></div>
  <div style="font-family: &quot;Museo Sans For Dell Regular&quot;; font-size: 11pt; margin-bottom: 12pt; margin-top: 12pt;"><span id="wwpID0E0DO0HA">To provide failure resiliency in a production deployment, at least two nodes should be used. Each node requires a Cassandra Server instance and local disk, and Cassandra must be configured with a </span><span style="font-style: italic;">replication factor</span> of 2 (RF=2). In this configuration, there is no need for mirrored disks since each node contains a complete copy of all data. At least one node must run a Doradus Server instance. This configuration is illustrated below:</div>
  <div style="font-family: &quot;Museo Sans For Dell Regular&quot;; font-size: 11pt; margin-bottom: 12pt; margin-top: 12pt; text-align: center;"><span id="wwpID0E0CO0HA">[[Image:Two_Node_Configuration-Doradus_Administration_-_v2.1.1.09.1.jpg]]</span></div>
  <div style="color: #4F81BD; font-family: &quot;Museo Sans For Dell Regular&quot;; font-size: 9pt; font-weight: bold; margin-bottom: 10pt; margin-top: 0pt; text-align: center;"><span id="wwpID0E0BO0HA">Figure 3 – Basic 2 Node Cluster: RF=2</span></div>
  <div style="font-family: &quot;Museo Sans For Dell Regular&quot;; font-size: 11pt; margin-bottom: 12pt; margin-top: 12pt;"><span id="wwpID0E0AO0HA">As shown, the single Doradus Server instance receives all REST commands and JMX requests. It is configured to connect to both Cassandra instances by defining the </span><span style="color: #4A442A; font-family: Consolas; font-size: 9.5pt; font-style: normal; font-weight: normal;">doradus.yaml</span> parameter <span style="color: #4A442A; font-family: Consolas; font-size: 9.5pt; font-style: normal; font-weight: normal;">dbhost</span> as a list of both addresses. Subsequently, Doradus distributes requests to both nodes, providing load balancing. The Cassandra nodes are configured with replication factor 2 (RF=2) so that each holds an identical set of data. Should one Cassandra server instance fail, all services can be supported by the surviving node.</div>
  <div style="font-family: &quot;Museo Sans For Dell Regular&quot;; font-size: 11pt; margin-bottom: 12pt; margin-top: 12pt;"><span id="wwpID0E06N0HA">Multiple Doradus instances can be used in the same cluster. A typical configuration is to deploy a Doradus instance on each node but configure it to use all Cassandra instances. In a 2-node configuration, this provides full redundancy and protection against any single process or machine failure. A fully redundant 2-node configuration is show below:</span></div>
  <div style="font-family: &quot;Museo Sans For Dell Regular&quot;; font-size: 11pt; margin-bottom: 12pt; margin-top: 12pt; text-align: center;"><span id="wwpID0E05N0HA">[[Image:Two_Node_Configuration-Doradus_Administration_-_v2.1.1.09.2.jpg]]</span></div>
  <div style="color: #4F81BD; font-family: &quot;Museo Sans For Dell Regular&quot;; font-size: 9pt; font-weight: bold; margin-bottom: 10pt; margin-top: 0pt; text-align: center;"><span id="wwpID0E04N0HA">Figure 4 – Fully Redundant 2 Node Cluster: RF=2</span></div>
  <div style="font-family: &quot;Museo Sans For Dell Regular&quot;; font-size: 11pt; margin-bottom: 12pt; margin-top: 12pt;"><span id="wwpID0E03N0HA">When multiple Doradus instances are used in the same cluster, they are </span><span style="font-style: italic;">peers</span>: any request can be sent to any instance. If a node fails, applications can redirect requests to any available instance. Doradus instances also communicate with each other to distribute background worker tasks and coordinate schema changes.</div>
</div>